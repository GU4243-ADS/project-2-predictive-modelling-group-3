---
title: "RandomForest"
author: "Sophie Beiers"
date: "2/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/project-2-predictive-modelling-group-3/doc")
```

## Load libraries
```{r}
if(!require("randomForest")){
  install.packages("randomForest")
}
if(!require("caret")){
  install.packages("caret")
}
if(!require("rfUtilities")){
  install.packages("rfUtilities")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
library(randomForest)
library(caret)
library(rfUtilities)
library(R.matlab)
```


##1.  HOG features
```{r}
features <- read.csv("../output/HOG_features.csv")
label.train <- read.csv("../output/label_train.csv",header=TRUE, as.is = TRUE)
label.train$x <- as.factor(label.train$x) # x is whether photo is dog or cat
```
```{r}
# training/testing
set.seed(2008)
in_train <- createDataPartition(y = label.train$x,
                                p = 3 / 4, 
                                list = FALSE)

training <- features[ in_train, ]
testing  <- features[-in_train, ]


y.train <- label.train[ in_train, 2 ]
y.test <- label.train[ -in_train, 2 ]

```

```{r}
## Error testing: finding best number of trees 
## Based on code by: Yuting Ma (ADS Spring 2016)
train<- function(df, label, N){
  i = proc.time()
  fit <- randomForest::randomForest(as.factor(y.train) ~ ., data = training, importance = TRUE, ntree = N)
  return(fit)

}

test<- function(rfmodel, df){
  pred <-predict(rfmodel, newdata = df)
  return (pred)
}

errors <- c()
N <- c(100, 200, 300, 400, 500, 600, 700, 800, 900)
# tried higher and lower, too. 2000 was too many tries
for(i in 1:9){

  fit <- train(training, y.train, N[i])
  pred_testing <- test(fit, testing)

  errors[i] <- mean(pred_testing != as.factor(y.test))
  cat(N[i]," trees has ", 1 - errors[i], "accuracy in prediction rate")

}

fit 
```

```{r}
# best model
1 - min(errors) ## best model uses 900 trees with a predication accuracy rate of 0.748. 

# double check
#pred_testing <- test(fit, testing)
#1 - mean(pred_testing != as.factor(y.test))

# how long does it take? 
system.time(randomForest(as.factor(y.train) ~ ., 
                   data = training, ntree = 900)) # 10.488 seconds elapsed 
```


## 2. Color Features
Not ideal. Took very long time with very little pay off. 
```{r}
# Training/Testing
features <- read.csv("../output/color_features.csv")

in_train <- createDataPartition(y = label.train$x,
                                p = 3 / 4, 
                                list = FALSE)

training <- features[ in_train, ]
testing  <- features[-in_train, ]


y.train <- label.train[ in_train, 2 ]
y.test <- label.train[ -in_train, 2 ]

```

```{r}
# Error testing to choose best number of trees for model 
train<- function(df, label, N){

  fit <- randomForest(as.factor(y.train) ~ ., data = training, importance = TRUE, ntree = N)
  return(fit)
}

test<- function(rfmodel, df){
  pred <-predict(rfmodel, newdata = df)
  return (pred)
}

errors <- c()
N <- c(100, 200, 300)

for(i in 1:3){

  fit <- train(training, y.train, N[i])

  pred_testing <- test(fit, testing)

  errors[i] <- mean(pred_testing != as.factor(y.test))
  cat(N[i]," trees has ", 1 - errors[i], "accuracy in prediction rate" )

}
```
```{r}
# best model
1 - min(errors) ## best model used 100 or 200 trees with .67 accuracy -- not good. 
# how long does it take? 
system.time(randomForest(as.factor(y.train) ~ ., 
                   data = training, ntree = 100)) ## 35.280  seconds is too long for this accuracy
```

## 3. SIFT Features
```{r}
# Training/Testing
features <- data_train

in_train <- createDataPartition(y = label.train$x,
                                p = 3 / 4, 
                                list = FALSE)

training <- features[ in_train, ]
testing  <- features[-in_train, ]


y.train <- label.train[ in_train, 2 ]
y.test <- label.train[ -in_train, 2 ]

```

```{r}
# Error testing to choose best model 
train<- function(df, label, N){

  fit <- randomForest(as.factor(y.train) ~ ., data = training, importance = TRUE, ntree = N)
  return(fit)
}

test<- function(rfmodel, df){
  pred <-predict(rfmodel, newdata = df)
  return (pred)
}

errors <- c()
N <- c(100, 200, 300, 400, 500, 600, 700, 800, 900)

for(i in 1:9){

  fit <- train(training, y.train, N[i])

  pred_testing <- test(fit, testing)

  errors[i] <- mean(pred_testing != as.factor(y.test))
  cat(N[i]," trees has ", 1 - errors[i], "accuracy in prediction rate" )

}
```

```{r}
# best model
1 - min(errors) ## best model uses 600 trees with a predication accuracy rate of 0.768. 
system.time(randomForest(as.factor(y.train) ~ ., 
                   data = training, ntree = 600)) # 9.675 seconds
```



## LBP Features
The below model is not ideal. Performed worse than using HOG or SIFT. 
```{r}
# Training/Testing 
features <- read.csv("../output/lbp-version2.csv")
features <- na.omit(features)

in_train <- createDataPartition(y = label.train$x,
                                p = 3 / 4, 
                                list = FALSE)

training <- features[ in_train, ]
testing  <- features[-in_train, ]


y.train <- label.train[ in_train, 2 ]
y.test <- label.train[ -in_train, 2 ]

```

```{r}
# Error testing to choose best model 
train<- function(df, label, N){

  fit <- randomForest(as.factor(y.train) ~ ., data = training, importance = TRUE, ntree = N)
  return(fit)
}

test<- function(rfmodel, df){
  pred <-predict(rfmodel, newdata = df)
  return (pred)
}

errors <- c()
N <- c(100, 200, 300, 400, 500, 600, 700, 800, 900)

for(i in 1:9){

  fit <- train(training, y.train, N[i])

  pred_testing <- test(fit, testing)
  
  na.action=na.exclude

  errors[i] <- mean(pred_testing != as.factor(y.test))
  cat(N[i]," trees has ", 1 - errors[i], "accuracy in prediction rate" )

}
```

```{r}
# best model
1 - min(errors) ## best model uses 200 trees with a predication accuracy rate of 0.672. 
system.time(randomForest(as.factor(y.train) ~ ., 
                   data = training, ntree = 200)) # 1.652 seconds elapsed
```


